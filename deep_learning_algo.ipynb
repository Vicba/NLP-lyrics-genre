{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to one :  lyrics to class pop or rap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51054 entries, 0 to 51053\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   lyric   51054 non-null  object\n",
      " 1   class   51054 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 797.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can't drink without thinkin' about you</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now Lil Pump flyin' private jet (Yuh)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No, matter fact, you ain't help me when I had ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And you could find me, I ain't hidin'</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From the way you talk to the way you move</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               lyric  class\n",
       "0             Can't drink without thinkin' about you      1\n",
       "1              Now Lil Pump flyin' private jet (Yuh)      0\n",
       "2  No, matter fact, you ain't help me when I had ...      0\n",
       "3              And you could find me, I ain't hidin'      0\n",
       "4          From the way you talk to the way you move      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    28885\n",
       "1    22169\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML bag-of-words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waarom? bag-of-words kan ook in machine learning maar is eigenlijk niet zo goed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df['lyric'], train_df['class'], test_size=0.2, random_state=42) # we use random state because there is no time related to the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove commas in lyrics\n",
    "train_df['lyric'] = train_df['lyric'].str.replace(',', '')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dealing with censored words (swear words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mothaf***a': 'Motherfucker', 'Motherf***ers': 'Motherfuckers', 'a**': 'ass', 'a**cheeks': 'asscheeks', 'a**hole': 'asshole', 'b****': 'bitch', 'b****es': 'bitches', 'b****in': 'bitching', 'b****y': 'bitchy', 'd***ed': 'damned', 'd***': 'dick', 'd***ie': 'doggie', 'd***ies': 'doggies', 'd***ory': 'doggory', 'd***y': 'doddy', 'f***': 'fack', 'fack***': 'facking', 'f***boy': 'fuckboy', 'f***boys': 'fuckboys', 'f***ed': 'fucked', 'f***er': 'fucker', 'f***ery': 'fuckery', 'f***in': 'fucking', 'f***ing': 'fucking', 'f***s': 'fucks', 'godd***it': 'goddammit', 'mothaf***a': 'motherfucker', 'mothaf***as': 'motherfuckers', 'mothaf***in': 'motherfucking', 'motherf***a': 'motherfucker', 'motherf***er': 'motherfucker', 'motherf***ers': 'motherfuckers', 'motherf***in': 'motherfucking', 'motherf***ing': 'motherfucking', 'muhf***as': 'motherfuckers', 'muhf***in': 'motherfucking', 'n****': 'nigga', 'n****s': 'niggas', 's***faced': 'shitfaced', 'p****': 'pussy', 's***': 'sick', 's***s': 'shits', 's***ted': 'shitted', 's***tin': 'shitting', 's***ty': 'shitty', 't***iana': 'tittiana', 't**s': 'tits', 't***': 'tits', 't***tarati': 'tittarati', 't***tie': 'tittie', 't***ties': 'titties', 't***ty': 'titty'}\n"
     ]
    }
   ],
   "source": [
    "# Define mapping of censored words to original words\n",
    "from censoredwords import censoredwords\n",
    "\n",
    "print(censoredwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping of censored words to original words\n",
    "from censoredwords import censoredwords\n",
    "\n",
    "# Reverse the censorship of the words\n",
    "for censored_word, original_word in censoredwords.items():\n",
    "    train_df['lyric'] = train_df['lyric'].str.replace(censored_word, original_word)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get rid of words like hidin' , thinkin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_apostrophes_with_g(word):\n",
    "    # check if the word ends with an apostrophe\n",
    "    if word.endswith(\"'\"):\n",
    "        # replace the apostrophe with \"g\"\n",
    "        word = word[:-1] + \"g\"\n",
    "    return word\n",
    "  \n",
    "def remove_apostrophes(text):\n",
    "    words = text.split()\n",
    "    words = [replace_apostrophes_with_g(word) for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "train_df[\"lyric\"] =  train_df[\"lyric\"].apply(remove_apostrophes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                   Can't drink without thinking about you\n",
      "1                    Now Lil Pump flying private jet (Yuh)\n",
      "2        No matter fact you ain't help me when I had no...\n",
      "3                     And you could find me I ain't hiding\n",
      "4                From the way you talk to the way you move\n",
      "                               ...                        \n",
      "51049    I told her pour me some more then she went rig...\n",
      "51050              Hit the ground and crawl to the dresser\n",
      "51051    Just keep breathing and breathing and breathin...\n",
      "51052         Down go the system long live the king (King)\n",
      "51053    If your mother knew all the things we do (From...\n",
      "Name: lyric, Length: 51054, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"lyric\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words] #all words that are not in stop_words\n",
    "    \n",
    "    # Stem the words = reducing to root form, example: running -> run\n",
    "    stemmer = SnowballStemmer(\"english\") \n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Join the words back into a single string\n",
    "    cleaned_text = ' '.join(stemmed_words)\n",
    "    return cleaned_text\n",
    "\n",
    "# De not is niet zo belangrijk\n",
    "# smaller words gehouden omdat dit veel voorkomt in rap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pas de preprocessing toe op de training- en validatiedata\n",
    "\n",
    "#The shape attribute for numpy arrays returns the dimensions of the array. If Y has n rows and m columns, then Y.shape is (n,m). So Y.shape[0] is n.\n",
    "for i in range(X_train.shape[0]):\n",
    "  X_train.iloc[i] = clean_text(X_train.iloc[i])\n",
    "\n",
    "for i in range(X_val.shape[0]):\n",
    "  X_val.iloc[i] = clean_text(X_val.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35972                                         man stay way\n",
       "28800                                                 pass\n",
       "5286                          think caus rememb first time\n",
       "25925    pop xan fifti thousand japan fifti thousand japan\n",
       "9889                                                  yeah\n",
       "                               ...                        \n",
       "11284                                               sick n\n",
       "44732                          nunca vieron caer nah nunca\n",
       "38158                                 everi time sit couch\n",
       "860                                         loos come come\n",
       "15795                                  kick call back come\n",
       "Name: lyric, Length: 40843, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean = X_train.copy()\n",
    "X_val_clean = X_val.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bag of words\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 1. Count the number of words\n",
    "count_vect = CountVectorizer()\n",
    "X_train_bag_of_words = count_vect.fit_transform(X_train) \n",
    "X_val_bag_of_words = count_vect.transform(X_val) # combinaties woorden en getallen zijn anders dan in training set\n",
    "\n",
    "# Take the frequency of the words intoaccount\n",
    "# wat irrelevant is krijgt mindere waarde dat andere woorden\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_bag_of_words)\n",
    "X_train_tf = tf_transformer.transform(X_train_bag_of_words)\n",
    "X_val_tf = tf_transformer.transform(X_val_bag_of_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.7654434262717772\n",
      "Best parameters : {'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victo\\.virtualenvs\\project-M_idfOFJ\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#logistic classifier on bag-of-words\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "paramaters = [{'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000,10000, 100000]}                                       \n",
    "             ] # penalisation parameter, smaller C = stronger penalisation\n",
    "                            \n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy', # accuracy is the metric to evaluate the model\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a second grid search, zooming in on values close to the best C-value from the first grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy :  0.7654434262717772\n",
      "Best parameters : {'C': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Victo\\.virtualenvs\\project-M_idfOFJ\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "paramaters = [{'C' : [int(x) for x in np.linspace(start = 9, stop = 11, num = 100)]}                                       \n",
    "             ] # penalisation parameter, smaller C = stronger penalisation\n",
    "                            \n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = paramaters,\n",
    "                           scoring = 'accuracy', # accuracy is the metric to evaluate the model\n",
    "                           cv = 4,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search = grid_search.fit(X_train_tf, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "\n",
    "print('Best accuracy : ', grid_search.best_score_)\n",
    "print('Best parameters :', grid_search.best_params_  )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameter is 10 and the accuracy stays the same of **76.54%**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL approach: LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM (Long Short-Term Memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(oov_token= True, num_words=500, split=' ')\n",
    "tokenizer.fit_on_texts(X_train_clean) # fit_on_texts functie bouwt een woordenboek \n",
    "\n",
    "X_train_tok = tokenizer.texts_to_sequences(X_train_clean)\n",
    "X_val_tok = tokenizer.texts_to_sequences(X_val_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences\n",
    "# pad_sequences is used to ensure that all sequences in a list have the same length. adding 0's\n",
    "# De maximale lengte is ingesteld op de lengte van de langste sequentie.\n",
    "X_train_tok = pad_sequences(X_train_tok)\n",
    "X_val_tok = pad_sequences(X_val_tok,maxlen=X_train_tok.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40843, 54)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tok.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the outcome is text, this padded sequence needs to be one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding is applied to convert the integer to a binary matrix (only 0s and 1s) because the model can't work with integers \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_train_class = to_categorical(y_train)\n",
    "\n",
    "y_val = encoder.transform(y_val)\n",
    "y_val_class = to_categorical(y_val)\n",
    "\n",
    "# 2 nodes als output layer => rap and pop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2043/2043 - 146s - loss: 0.5330 - accuracy: 0.7318 - val_loss: 0.5074 - val_accuracy: 0.7489 - 146s/epoch - 71ms/step\n",
      "Epoch 2/100\n",
      "2043/2043 - 154s - loss: 0.5016 - accuracy: 0.7519 - val_loss: 0.5052 - val_accuracy: 0.7481 - 154s/epoch - 75ms/step\n",
      "Epoch 3/100\n",
      "2043/2043 - 128s - loss: 0.4887 - accuracy: 0.7603 - val_loss: 0.5034 - val_accuracy: 0.7511 - 128s/epoch - 63ms/step\n",
      "Epoch 4/100\n",
      "2043/2043 - 135s - loss: 0.4761 - accuracy: 0.7642 - val_loss: 0.4968 - val_accuracy: 0.7524 - 135s/epoch - 66ms/step\n",
      "Epoch 5/100\n",
      "2043/2043 - 133s - loss: 0.4629 - accuracy: 0.7717 - val_loss: 0.4936 - val_accuracy: 0.7635 - 133s/epoch - 65ms/step\n",
      "Epoch 6/100\n",
      "2043/2043 - 124s - loss: 0.4478 - accuracy: 0.7831 - val_loss: 0.4919 - val_accuracy: 0.7669 - 124s/epoch - 61ms/step\n",
      "Epoch 7/100\n",
      "2043/2043 - 134s - loss: 0.4336 - accuracy: 0.7909 - val_loss: 0.4982 - val_accuracy: 0.7700 - 134s/epoch - 66ms/step\n",
      "Epoch 8/100\n",
      "2043/2043 - 117s - loss: 0.4194 - accuracy: 0.8000 - val_loss: 0.4804 - val_accuracy: 0.7726 - 117s/epoch - 57ms/step\n",
      "Epoch 9/100\n",
      "2043/2043 - 140s - loss: 0.4034 - accuracy: 0.8098 - val_loss: 0.4834 - val_accuracy: 0.7777 - 140s/epoch - 68ms/step\n",
      "Epoch 10/100\n",
      "2043/2043 - 172s - loss: 0.3855 - accuracy: 0.8192 - val_loss: 0.4832 - val_accuracy: 0.7809 - 172s/epoch - 84ms/step\n",
      "Epoch 11/100\n",
      "2043/2043 - 118s - loss: 0.3702 - accuracy: 0.8289 - val_loss: 0.4879 - val_accuracy: 0.7820 - 118s/epoch - 58ms/step\n",
      "Epoch 12/100\n",
      "2043/2043 - 120s - loss: 0.3532 - accuracy: 0.8363 - val_loss: 0.4923 - val_accuracy: 0.7886 - 120s/epoch - 59ms/step\n",
      "Epoch 13/100\n",
      "2043/2043 - 117s - loss: 0.3392 - accuracy: 0.8435 - val_loss: 0.4989 - val_accuracy: 0.7842 - 117s/epoch - 57ms/step\n",
      "Epoch 14/100\n",
      "2043/2043 - 126s - loss: 0.3247 - accuracy: 0.8527 - val_loss: 0.5082 - val_accuracy: 0.7885 - 126s/epoch - 62ms/step\n"
     ]
    }
   ],
   "source": [
    "# we use word embedding to represent words as vectors in a high-dimensional space so that semantically similar words are mapped to nearby points\n",
    "model = Sequential()\n",
    "# the neural network that performs overall task\n",
    "model.add(Embedding(input_dim=2000,output_dim =128, input_length = X_train_tok.shape[1])) \n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # when adding new LSTM, add return_sequences=True\n",
    "model.add(Dense(2,activation='sigmoid')) # whe use sigmoid because we have 2 classes as (output layer) \n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy']) # binary_crossentropy because we have 2 classes, adam because it is a binary classification problem , we kijken naar de accuracy\n",
    "\n",
    "early_stopping =  EarlyStopping(patience=6,  restore_best_weights=True)\n",
    "history = model.fit(X_train_tok, y_train_class, epochs = 100, batch_size=16, verbose = 2, validation_split= 0.2,\n",
    "                   callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy:  0.7885910272598267\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = max(history.history['val_accuracy'])\n",
    "print(\"Best validation accuracy: \", best_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation accuracy is **78.85%**. This is higher than the validation score of the ML algorithm (76.54), so we prefer the LSTM model. \n",
    "Predict the results for the test set and take a look at the classification report, use the best model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/320 [==============================] - 5s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions_test = np.argmax(model.predict(X_val_tok), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79      5781\n",
      "           1       0.73      0.71      0.72      4430\n",
      "\n",
      "    accuracy                           0.76     10211\n",
      "   macro avg       0.76      0.75      0.75     10211\n",
      "weighted avg       0.76      0.76      0.76     10211\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_val, predictions_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data (dit pas doen als je het beste algoritme gevonden hebt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove commas in lyrics\n",
    "train_df['lyric'] = train_df['lyric'].str.replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mapping of censored words to original words\n",
    "from censoredwords import censoredwords\n",
    "\n",
    "# Reverse the censorship of the words\n",
    "for censored_word, original_word in censoredwords.items():\n",
    "    train_df['lyric'] = train_df['lyric'].str.replace(censored_word, original_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"lyric\"] =  train_df[\"lyric\"].apply(remove_apostrophes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the lyrics column\n",
    "\n",
    "test_df['cleaned_lyrics'] = test_df['lyric'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "test_seq = tokenizer.texts_to_sequences(test_df['cleaned_lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 34s 403ms/step\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to a fixed length\n",
    "test_pad = pad_sequences(test_seq, maxlen=2000)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to binary values using 0.5\n",
    "binary_predictions = y_pred.round()\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame({'Id': test_df['id'], 'Prediction': binary_predictions[:, 1], 'Lyrics': test_df['lyric']})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "result_df.to_csv('./data/results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-M_idfOFJ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
